import os

import chainlit as cl
from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain
from langchain_community.chat_models import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.memory.buffer import ConversationBufferMemory

from prompt import PROMPT


def load_llm():
    """
    Load the Language Learning Model (LLM).

    This function initializes and returns an instance of the OpenAI GPT-3.5 Turbo model
    configured for streaming responses with a temperature setting of 0.
    """
    llm = ChatOpenAI(
        model='gpt-3.5-turbo-0125',
        temperature=0,
        streaming=True
    )
    return llm


def retrieval_qa_chain(llm, db, memory):
    """
    Create a Retrieval-based QA chain with sources.

    This function creates a question-answering chain using a given language model,
    vector database, and conversation memory. It configures the chain to retrieve
    source documents and utilize a specific prompt for generating responses.

    Args:
        llm: The language learning model instance.
        db: The FAISS vector database instance.
        memory: The conversation buffer memory instance.

    Returns:
        An instance of the RetrievalQAWithSourcesChain.
    """
    qa_chain = RetrievalQAWithSourcesChain.from_chain_type(
        llm=llm,
        chain_type='stuff',
        memory=memory,
        retriever=db.as_retriever(search_kwargs={'k': 1}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain


def qa_bot():
    """
    Initialize the QA bot.

    This function loads the embeddings, vector database, language model, and
    conversation memory to set up a QA bot. It returns the configured QA chain.

    Returns:
        The configured QA chain instance.
    """
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2",
                                       model_kwargs={'device': 'cpu'})
    db = FAISS.load_local(os.getenv("DB_FAISS_PATH"), embeddings, allow_dangerous_deserialization=True)
    llm = load_llm()
    conversation_memory = ConversationBufferMemory(
        memory_key="chat_history",
        output_key="answer",
        max_len=50,
        return_messages=True,
    )
    qa = retrieval_qa_chain(llm, db, conversation_memory)
    return qa


def final_result(query):
    """
    Get the final result from the QA bot for a given query.

    This function initializes the QA bot and uses it to process the input query,
    returning the response generated by the bot.

    Args:
        query: The input query string.

    Returns:
        The response from the QA bot.
    """
    qa_result = qa_bot()
    response = qa_result({'query': query})
    return response


@cl.on_chat_start
async def start():
    """
    Handle the start of a chat session.

    This asynchronous function initializes the QA bot, sends an avatar and a welcome
    message to the user, and stores the QA chain in the user session.
    """
    chain = qa_bot()
    avatar = cl.Avatar(
        name="Swim Buddy Bot",
        path="public/avatars/swim-buddy-bot.png"
    )
    await avatar.send()
    msg = cl.Message(content="Hi, Welcome to Swim Buddy! üèäüèä")
    await msg.send()
    cl.user_session.set("chain", chain)


@cl.on_message
async def main(message: cl.Message):
    """
    Handle incoming messages in the chat session.

    This asynchronous function retrieves the QA chain from the user session,
    creates an asynchronous callback handler for streaming the final answer,
    and processes the incoming message content using the QA chain.

    Args:
        message: The incoming message from the chat session.
    """
    chain = cl.user_session.get("chain")
    cb = cl.AsyncLangchainCallbackHandler(
        stream_final_answer=True, answer_prefix_tokens=["FINAL", "ANSWER"]
    )
    cb.answer_reached = True

    await chain.acall(message.content, callbacks=[cb])
